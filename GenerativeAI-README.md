# Thyroid Disease Likelihood Project
## Generative AI Integration Documentation

## Overview

This project integrates OpenAI's GPT-4.1 model as an optional enhancement to a hierarchical machine learning system for thyroid disease classification. The Generative AI component provides personalized, human-readable interpretations of complex medical classification results, making the system more accessible to both healthcare professionals and patients.

**Key Integration Points:**
- **Model:** OpenAI GPT-4.1 (accessed via OpenAI Python client)
- **Primary Function:** Natural language explanation of ML predictions
- **Implementation:** Optional feature activated when `OPENAI_API_KEY` is configured in `.env` file
- **Architecture:** Standalone `AIInsightsGenerator` class in `src/utils/ai_insights.py`
- **Integration Point:** Called in `display_prediction_results()` function in `src/app.py`
- **Output:** Four structured sections (interpretation, risk_factors, recommendations, education)


## Purpose

### Why Generative AI?

The hierarchical machine learning models produce accurate classifications but generate technical outputs that may be difficult for non-experts to interpret:

- **Problem:** ML outputs like "Stage 1: Sick (92.3% confidence), Stage 2: G - Compensated Hypothyroid (85.1% confidence)" are accurate but lack context
- **Solution:** GPT-4.1 translates these technical results into four types of actionable, compassionate insights
- **Value Add:** Bridges the gap between ML predictions and patient understanding without replacing medical advice

### Use Case Justification

Generative AI was chosen specifically for:

1. **Result Interpretation:** Explaining classification results and diagnosis codes in plain language
2. **Risk Factor Analysis:** Identifying which lab values and features influenced the classification
3. **Practical Recommendations:** Providing actionable next steps with emphasis on consulting healthcare professionals
4. **Health Education:** Offering brief educational information about thyroid health relevant to the results


## Technical Architecture

### System Overview

```mermaid
flowchart TD
    A[Streamlit Application<br/>src/app.py] --> B[User Input Form]
    B --> C[ThyroidPredictor]
    
    C --> D[Stage 1 Classifier<br/>Healthy/Sick]
    D --> E{Result}
    
    E -->|Not Sick| F[Display: Negative<br/>Skip Stage 2]
    E -->|Sick| G[Stage 2 Classifier<br/>Specific Diagnosis]
    
    G --> H[Feature Importance<br/>Calculation]
    H --> I[Prediction Dictionary]
    
    I --> J{OPENAI_API_KEY<br/>Configured?}
    
    J -->|No| K[Display ML Results Only]
    J -->|Yes| L[AIInsightsGenerator]
    
    L --> M[generate_insights]
    M --> N[OpenAI API Call<br/>GPT-4.1]
    N --> O[_parse_response]
    O --> P[Insights Dictionary]
    
    P --> Q[Display in Expanders:<br/>- Result Interpretation<br/>- Risk Factors<br/>- Recommendations<br/>- Education]
    
    K --> R[Complete Display]
    Q --> R
    
    style A fill:#E3F2FD
    style L fill:#FFF3E0
    style N fill:#FFEBEE
    style Q fill:#C8E6C9
```

### Component Architecture

```mermaid
classDiagram
    class AIInsightsGenerator {
        -api_key: str
        -client: OpenAI or None
        +__init__(api_key=None)
        +generate_insights(patient_data, prediction, top_features)
        -_parse_response(response_text)
    }
    
    class StreamlitApp {
        +main()
        +prediction_tab()
        +display_prediction_results()
        +manual_input_form()
    }
    
    class ThyroidPredictor {
        +predict()
        +get_feature_importance()
    }
    
    class OpenAIAPI {
        <<external>>
        +chat.completions.create()
    }
    
    StreamlitApp --> ThyroidPredictor : uses
    StreamlitApp --> AIInsightsGenerator : uses conditionally
    AIInsightsGenerator --> OpenAIAPI : calls if client exists
    
    note for AIInsightsGenerator "Location: utils/ai_insights.py\nOptional: Only works if API key configured"
    note for OpenAIAPI "Model: gpt-4.1\nTemperature: 0.7\nMax Tokens: 1500"
```

---

## Implementation Details

### 1. Class Structure

**File:** `src/utils/ai_insights.py`

The `AIInsightsGenerator` class has three main components:

#### Initialization (`__init__`)

```python
def __init__(self, api_key=None):
    self.api_key = api_key or os.getenv('OPENAI_API_KEY')
    self.client = None
    
    if self.api_key:
        try:
            self.client = OpenAI(api_key=self.api_key)
        except Exception as e:
            print(f"Error initializing OpenAI client: {e}")
            self.client = None
```

**Design Decisions:**
- **Graceful Degradation:** If no API key is provided or initialization fails, `self.client` remains `None`
- **Environment Variable Priority:** Checks for `OPENAI_API_KEY` in environment (loaded from `.env` via `python-dotenv`)
- **Error Handling:** Catches initialization errors without crashing the application

#### Main Method (`generate_insights`)

```python
def generate_insights(self, patient_data, prediction, top_features):
    if not self.client:
        return {
            'error': True,
            'interpretation': 'OpenAI API key not configured.'
        }
    
    try:
        # Data extraction and validation (lines 49-92)
        # Prompt construction (lines 94-123)
        # API call (lines 125-140)
        # Response parsing (line 146)
        return insights
    except Exception as e:
        return {
            'error': True,
            'interpretation': f'Unable to generate insights at this time.'
        }
```

**Key Features:**
- Early return if client not initialized (prevents API call without credentials)
- Comprehensive error handling with try-except
- Returns error dictionary on failure (doesn't crash the app)

#### Response Parser (`_parse_response`)

```python
def _parse_response(self, response_text):
    sections = {
        'interpretation': '',
        'risk_factors': '',
        'recommendations': '',
        'education': ''
    }
    
    # Line-by-line parsing with keyword detection (lines 179-225)
    # State machine tracks current section
    # Returns dictionary with four sections
```

**Parsing Strategy:**
- Detects section headers using keyword matching
- Uses state machine to track current section
- Accumulates content line-by-line
- Falls back to putting everything in 'interpretation' if parsing fails

### 2. Data Preparation and Validation

The code implements extensive data validation before sending to GPT-4.1:

**Patient Data Processing:**
```python
# Age
age = str(patient_data.get('age') or 'Unknown')

# Sex conversion
sex_val = patient_data.get('sex')
sex = 'Female' if sex_val == 1 else 'Male' if sex_val == 0 else 'Unknown'

# Lab values with null handling
tsh = patient_data.get('TSH')
tsh_str = f"{float(tsh):.2f}" if tsh is not None else 'Not measured'
```

**Prediction Data Processing:**
```python
# Stage predictions
stage1 = str(prediction.get('stage1_prediction') or 'Unknown')
stage2 = str(prediction.get('stage2_prediction') or 'Not classified')

# Confidence values
confidence1_raw = prediction.get('stage1_confidence')
confidence1 = f"{float(confidence1_raw):.1%}" if confidence1_raw is not None else "Not available"

# Risk and diagnosis
risk_level = str(prediction.get('risk_level') or 'Unknown')
final_diagnosis = str(prediction.get('final_diagnosis') or 'Unknown')
```

**Feature Importance Processing:**
```python
feature_list = []
if top_features and len(top_features) > 0:
    for item in top_features[:5]:  # Only top 5 features
        try:
            if isinstance(item, (list, tuple)) and len(item) >= 2:
                feat, importance = item[0], item[1]
                if feat is not None and importance is not None:
                    feat_str = str(feat).upper()
                    imp_str = f"{float(importance):.3f}"
                    feature_list.append(f"- {feat_str}: {imp_str}")
        except (ValueError, TypeError, IndexError):
            continue

features_str = "\n".join(feature_list) if feature_list else "No feature importance data available"
```

**Validation Strategy:**
- Null-safe: All values checked before conversion
- Type-safe: Defensive type conversion with fallbacks
- Format-consistent: Percentages formatted as `.1%`, decimals as `.2f` or `.3f`
- Graceful: Missing data labeled explicitly ("Not measured", "Unknown")

---

## AI Functionality and Features

### 1. Prompt Construction

The system builds a structured prompt containing:

#### Patient Information
- **Age:** Extracted from patient_data
- **Sex:** Binary value (0/1) converted to "Male"/"Female"
- **Lab Values with Normal Ranges:**
  - TSH: mIU/L (Normal: 0.4-4.0)
  - T3: nmol/L (Normal: 0.8-2.0)
  - TT4: nmol/L (Normal: 60-150)

#### Classification Results
- **Stage 1:** Initial screening result (Healthy/Sick) with confidence percentage
- **Stage 2:** Specific diagnosis code with confidence percentage
- **Final Diagnosis:** Diagnosis code (A-T or combinations)
- **Risk Level:** Low/Medium/High assessment

#### Feature Importance
- **Top 5 Features:** Most influential features from model
- **Importance Scores:** Float values (0-1 scale) formatted to 3 decimal places

### 2. Complete Prompt Template

```python
prompt = f"""You are a medical AI assistant helping to explain thyroid disease screening results. 
Provide clear, compassionate, and accurate insights based on this patient's data.

Patient Information:
- Age: {age} years
- Sex: {sex}
- TSH: {tsh_str} mIU/L (Normal: 0.4-4.0)
- T3: {t3_str} nmol/L (Normal: 0.8-2.0)
- TT4: {tt4_str} nmol/L (Normal: 60-150)

Classification Results:
- Stage 1 (Initial Screening): {stage1}
- Stage 1 Confidence: {confidence1}
- Stage 2 Classification: {stage2}
- Stage 2 Confidence: {confidence2}
- Final Diagnosis: {final_diagnosis}
- Risk Level: {risk_level}

Top Contributing Features:
{features_str}

Please provide insights in all of these areas:

RESULT INTERPRETATION: Explain what these results mean or the overview of the results to a patient. If the patient is positive with thyroid disease, explain the diagnosis classification.
RISK FACTORS: Identify the key factors that influenced this classification.
RECOMMENDATIONS: Suggest practical next steps (emphasize consulting healthcare professionals).
EDUCATION: Provide brief educational information about thyroid health relevant to these results.

Format your response with clear section headers and use markdown formatting."""
```

### 3. System Prompt

```python
{
    "role": "system",
    "content": "You are a helpful medical AI assistant specializing in thyroid health. Provide clear, accurate, and compassionate explanations. Always emphasize that these are screening results and not a diagnosis, and that patients should consult healthcare professionals."
}
```

**Key Elements:**
- **Role Definition:** Medical AI assistant specializing in thyroid health
- **Tone Guidance:** Clear, accurate, and compassionate
- **Safety Emphasis:** Screening vs. diagnosis distinction
- **Professional Referral:** Always recommend consulting healthcare professionals

### 4. API Configuration

```python
response = self.client.chat.completions.create(
    model="gpt-4.1",
    messages=[
        {"role": "system", "content": "..."},
        {"role": "user", "content": prompt}
    ],
    temperature=0.7,
    max_tokens=1500
)
```

**Parameter Justification:**
- **model:** `"gpt-4.1"` - Latest GPT-4 model with medical reasoning capabilities
- **temperature:** `0.7` - Balances creativity (natural language) with consistency (factual accuracy)
- **max_tokens:** `1500` - Allows comprehensive 4-section response (~800-1200 tokens typical output)

---

## Data Flow and Processing

### Complete Data Flow Sequence

```mermaid
sequenceDiagram
    participant User
    participant Streamlit as Streamlit UI
    participant Predictor as ThyroidPredictor
    participant Display as display_prediction_results
    participant AIGen as AIInsightsGenerator
    participant OpenAI as OpenAI API
    
    User->>Streamlit: Enter patient data & submit
    Streamlit->>Predictor: predict(patient_data)
    Predictor->>Predictor: Stage 1 classification
    
    alt Stage 1 = "Sick"
        Predictor->>Predictor: Stage 2 classification
        Predictor->>Predictor: Calculate feature importance
        Predictor-->>Streamlit: prediction dict + top_features
        
        Streamlit->>Display: display_prediction_results(prediction, patient_data, ai_generator)
        Display->>Display: Render ML results
        Display->>Display: Render feature importance chart
        
        alt ai_generator.client exists
            Display->>AIGen: generate_insights(patient_data, prediction, top_features)
            AIGen->>AIGen: Validate and format data
            AIGen->>AIGen: Construct prompt (patient info + results)
            AIGen->>OpenAI: chat.completions.create(model="gpt-4.1", ...)
            OpenAI-->>AIGen: GPT-4 response (markdown text)
            AIGen->>AIGen: _parse_response() - extract 4 sections
            AIGen-->>Display: insights dict {interpretation, risk_factors, recommendations, education}
            Display->>Display: clean_insight() - remove separators
            Display->>User: Display 4 expanders with AI insights
        else No API key configured
            Display->>User: Display info message: "AI insights disabled"
        end
    else Stage 1 = "Not Sick"
        Predictor-->>Streamlit: prediction dict (no Stage 2)
        Streamlit->>Display: display_prediction_results(prediction, patient_data, ai_generator)
        Display->>User: Display "Negative Thyroid Disease"
    end
```

### Input Processing Pipeline

```mermaid
flowchart TD
    A[Patient Data + Prediction + Top Features] --> B[Null Checks]
    B --> C[Type Conversions]
    C --> D[String Formatting]
    D --> E[Validation]
    
    E --> F{All Data Valid?}
    F -->|Yes| G[Build Prompt String]
    F -->|No| H[Use Fallback Values]
    
    H --> G
    G --> I[API Request Object]
    
    style A fill:#FFEBEE
    style E fill:#FFF9C4
    style G fill:#C8E6C9
    style I fill:#BBDEFB
```

### Output Processing Pipeline

```mermaid
flowchart TD
    A[GPT-4 Response<br/>Markdown Text] --> B[Split by Newline]
    B --> C[State Machine Parser]
    
    C --> D{Detect Header?}
    D -->|"result interpretation"| E1[current_section = 'interpretation']
    D -->|"risk factors"| E2[current_section = 'risk_factors']
    D -->|"recommendations"| E3[current_section = 'recommendations']
    D -->|"education"| E4[current_section = 'education']
    D -->|No header| F[Append to current_content]
    
    E1 --> G[Save previous section]
    E2 --> G
    E3 --> G
    E4 --> G
    
    F --> H[Continue loop]
    G --> H
    
    H --> I{More Lines?}
    I -->|Yes| C
    I -->|No| J[Save last section]
    
    J --> K{Any Sections Populated?}
    K -->|Yes| L[Return sections dict]
    K -->|No| M[Put all in 'interpretation']
    
    M --> L
    L --> N[UI Display]
    
    style A fill:#E3F2FD
    style C fill:#FFF9C4
    style L fill:#C8E6C9
```

---

## Prompt Engineering Strategy

### Prompt Structure Overview

```mermaid
graph TB
    A[Complete Prompt] --> B[System Message]
    A --> C[User Message]
    
    B --> B1["Role: Medical AI Assistant<br/>Specialty: Thyroid Health"]
    B --> B2["Tone: Clear, Accurate, Compassionate"]
    B --> B3["Safety: Screening vs Diagnosis"]
    B --> B4["Referral: Consult Professionals"]
    
    C --> D[Patient Information Block]
    C --> E[Classification Results Block]
    C --> F[Feature Importance Block]
    C --> G[Output Instructions Block]
    
    D --> D1["Age: {age} years"]
    D --> D2["Sex: {sex}"]
    D --> D3["TSH: {tsh_str} mIU/L<br/>Normal: 0.4-4.0"]
    D --> D4["T3: {t3_str} nmol/L<br/>Normal: 0.8-2.0"]
    D --> D5["TT4: {tt4_str} nmol/L<br/>Normal: 60-150"]
    
    E --> E1["Stage 1: {stage1}<br/>Confidence: {confidence1}"]
    E --> E2["Stage 2: {stage2}<br/>Confidence: {confidence2}"]
    E --> E3["Final Diagnosis: {final_diagnosis}"]
    E --> E4["Risk Level: {risk_level}"]
    
    F --> F1["Top 5 Features with<br/>Importance Scores 0-1"]
    
    G --> G1["RESULT INTERPRETATION:<br/>Explain results to patient"]
    G --> G2["RISK FACTORS:<br/>Key influencing factors"]
    G --> G3["RECOMMENDATIONS:<br/>Practical next steps"]
    G --> G4["EDUCATION:<br/>Brief thyroid health info"]
    
    style A fill:#E1BEE7
    style B fill:#F8BBD0
    style C fill:#BBDEFB
    style D fill:#C5E1A5
    style E fill:#FFE082
    style F fill:#FFCCBC
    style G fill:#B2DFDB
```

### Design Principles

1. **Structured Sections:** Explicitly requests four distinct insight types using ALL CAPS headers
2. **Context-Rich:** Provides comprehensive patient data including normal ranges for interpretation
3. **Output Format:** Requests markdown formatting with clear section headers
4. **Safety First:** System prompt emphasizes "screening results and not a diagnosis"
5. **Tone Guidance:** Requests "clear, compassionate, and accurate" explanations

### Section-Specific Instructions

| Section | Goal | Audience | Content |
|---------|------|----------|---------|
| **Result Interpretation** | Explain classification in plain language | Patients or healthcare providers | Overview of findings, diagnosis code explanation |
| **Risk Factors** | Identify key contributing features | Clinically-minded users | Which lab values or features influenced the classification |
| **Recommendations** | Provide actionable next steps | Patients seeking guidance | Practical advice with emphasis on professional consultation |
| **Education** | Build health literacy | General public | Brief thyroid health information relevant to results |


### Configuration Security

```python
# âœ… SECURE: Environment variable
load_dotenv()
self.api_key = os.getenv('OPENAI_API_KEY')

# NOT USED: Hardcoded
# self.api_key = "sk-..."
```

## User Experience Integration

### Streamlit UI Integration

#### 1. Initialization in Session State

```python
# In app.py main()
if 'ai_generator' not in st.session_state:
    st.session_state.ai_generator = None

# During initialization
ai_generator = AIInsightsGenerator()
st.session_state.ai_generator = ai_generator
```

#### 2. Conditional Display Logic

```python
# In display_prediction_results() - line 715
if ai_generator and ai_generator.client:
    with st.spinner("ðŸ”„ Generating personalized insights..."):
        insights = ai_generator.generate_insights(
            patient_data,
            prediction,
            top_features
        )
    
    if 'error' not in insights or not insights.get('error'):
        # Display 4 expanders with insights
        with st.expander("ðŸ“‹ Result Interpretation", expanded=True):
            st.markdown(clean_insight(insights["interpretation"]))
        
        with st.expander("âš ï¸ Key Risk Factors", expanded=True):
            st.markdown(clean_insight(insights["risk_factors"]))
        
        with st.expander("ðŸ’¡ Recommendations", expanded=True):
            st.markdown(clean_insight(insights["recommendations"]))
        
        with st.expander("ðŸ“š Educational Information", expanded=True):
            st.markdown(clean_insight(insights["education"]))
    else:
        st.info("ðŸ’¡ AI insights unavailable. " + insights.get('interpretation', ''))
else:
    st.info("ðŸ’¡ AI insights are currently disabled. Add your OPENAI_API_KEY to the .env file...")
```

#### 3. Text Cleaning Function

```python
def clean_insight(text):
    """Remove trailing newlines and separators from insight text."""
    if text:
        if text.endswith('---'):
            text = text[:-3].rstrip()
            text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)
        return text
    return text
```

**Purpose:** Removes markdown artifacts (separators, redundant headers) from GPT-4 output before display

### User Workflow

```mermaid
flowchart TD
    A[User Opens App] --> B[Fill Patient Form]
    B --> C[Click 'Generate Prediction']
    C --> D[View ML Results]
    
    D --> E{API Key Configured?}
    
    E -->|No| F[See Info Message:<br/>'AI insights disabled']
    E -->|Yes| G[Spinner Appears:<br/>'Generating insights...']
    
    G --> H[API Call to GPT-4.1]
    H --> I{Success?}
    
    I -->|Yes| J[4 Expanders Render:<br/>- Result Interpretation expanded<br/>- Risk Factors expanded<br/>- Recommendations collapsed<br/>- Education collapsed]
    I -->|No| K[Info Message:<br/>'AI insights unavailable']
    
    F --> L[User Can Still:<br/>- See ML predictions<br/>- View feature importance<br/>- Read standard recommendations]
    K --> L
    J --> M[User Reads AI Insights<br/>+ All ML Results]
    
    style A fill:#E3F2FD
    style G fill:#FFF9C4
    style J fill:#C8E6C9
    style L fill:#FFE0B2
```

---

## Code Structure and Integration

### File Structure

```mermaid
graph TD
    A[project_root/src] --> B[app.py]
    A --> C[utils/]
    A --> D[.env]
    A --> E[.gitignore]
    A --> F[requirements.txt]
    
    C --> C1[ai_insights.py]
    C --> C2[data_processor.py]
    C --> C3[model_loader.py]
    C --> C4[predictor.py]
    
    B:::main
    C1:::ai
    D:::config
    
    classDef main fill:#BBDEFB,stroke:#1976D2,stroke-width:3px
    classDef ai fill:#FFF9C4,stroke:#F57C00,stroke-width:2px
    classDef config fill:#FFCCBC,stroke:#E64A19,stroke-width:2px
    
    style A fill:#E1BEE7,stroke:#7B1FA2,stroke-width:3px
```

### Integration Points

**1. Import Statement (app.py line 19):**
```python
from utils.ai_insights import AIInsightsGenerator
```

**2. Initialization (app.py line 318):**
```python
ai_generator = AIInsightsGenerator()
```

**3. Session State Storage (app.py line 345):**
```python
st.session_state.ai_generator = ai_generator
```

**4. Function Parameter (app.py line 396):**
```python
def prediction_tab(processor, predictor, ai_generator):
```

**5. Actual Usage (app.py line 715-749):**
```python
if ai_generator and ai_generator.client:
    insights = ai_generator.generate_insights(patient_data, prediction, top_features)
    # Display insights in expanders
```

### Key Dependencies

```
openai>=1.0.0          # OpenAI Python client
python-dotenv>=0.19.0  # Load .env file
streamlit>=1.12.0      # Web UI framework
pandas>=1.3.0          # Data manipulation
numpy>=1.21.0          # Numerical operations
plotly>=5.0.0          # Interactive charts
```

### Environment Configuration

```bash
# .env file (in project root)
OPENAI_API_KEY=sk-your-api-key-here
```

**Setup Instructions:**
1. Create `.env` file in project root directory
2. Add line: `OPENAI_API_KEY=your-actual-key`
3. Ensure `.env` is in `.gitignore`
4. Restart Streamlit application
5. AI insights will automatically be available

